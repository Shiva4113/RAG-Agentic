{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/neural-maze/agentic_patterns.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHZNrwhyJtbq",
        "outputId": "1f77ebcb-d7e8-4f94-db0b-7244d4a6e3df"
      },
      "id": "eHZNrwhyJtbq",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'agentic_patterns' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd agentic_patterns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev0Tz3PzJwVR",
        "outputId": "368678eb-5e24-4883-f90a-464edc6b635e"
      },
      "id": "Ev0Tz3PzJwVR",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/agentic_patterns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq==0.10.0 jupyter==1.0.0 python-dotenv==1.0.1 colorama==0.4.6 types-colorama==0.4.15.20240311 graphviz==0.20.3 httpx==0.27.2 json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrOcbc1DJxit",
        "outputId": "593a109d-55e1-4d30-f023-a8c78dc19353"
      },
      "id": "hrOcbc1DJxit",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq==0.10.0 in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: python-dotenv==1.0.1 in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: colorama==0.4.6 in /usr/local/lib/python3.11/dist-packages (0.4.6)\n",
            "Requirement already satisfied: types-colorama==0.4.15.20240311 in /usr/local/lib/python3.11/dist-packages (0.4.15.20240311)\n",
            "Requirement already satisfied: graphviz==0.20.3 in /usr/local/lib/python3.11/dist-packages (0.20.3)\n",
            "Requirement already satisfied: httpx==0.27.2 in /usr/local/lib/python3.11/dist-packages (0.27.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d105473-c0c6-4de6-acfb-ccf3054fd1a0",
      "metadata": {
        "id": "7d105473-c0c6-4de6-acfb-ccf3054fd1a0"
      },
      "source": [
        "# Tool Pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e9c48d-cac5-48a1-a6ff-e44b618b92c9",
      "metadata": {
        "id": "39e9c48d-cac5-48a1-a6ff-e44b618b92c9"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "As you may already know, the information stored in LLM weights is (usually) ùêßùê®ùê≠ ùêûùêßùê®ùêÆùê†ùê° to give accurate and insightful answers to our questions.\n",
        "\n",
        "That's why we need to provide the LLM with ways to access the outside world. üåç\n",
        "\n",
        "In practice, you can build tools for whatever you want (at the end of the day they are just functions the LLM can use), from a tool that let's you access Wikipedia, another to analyse the content of YouTube videos or calculate difficult integrals using Wolfram Alpha.\n",
        "\n",
        "The second pattern we are going to implement is the **tool pattern**.\n",
        "\n",
        "In this notebook, you'll learn how **tools** actually work. This is the **second lesson** of the \"Agentic Patterns from Scratch\" series. Take a look at the first lesson if you haven't!\n",
        "\n",
        "* [First Lesson: The Reflection Pattern](https://github.com/neural-maze/agentic_patterns/blob/main/notebooks/reflection_pattern.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6eb2bab-9a5b-4c92-b23a-18f757d44c06",
      "metadata": {
        "id": "a6eb2bab-9a5b-4c92-b23a-18f757d44c06"
      },
      "source": [
        "## A simple function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148df24a-4ac5-4d3d-9860-8ff0e7ed7c90",
      "metadata": {
        "id": "148df24a-4ac5-4d3d-9860-8ff0e7ed7c90"
      },
      "source": [
        "Take a look at this function üëá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1c851271-9b5a-4b48-a0e0-bf889cfb303b",
      "metadata": {
        "id": "1c851271-9b5a-4b48-a0e0-bf889cfb303b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def get_current_weather(location: str, unit: str):\n",
        "\t\"\"\"\n",
        "\tGet the current weather in a given location\n",
        "\n",
        "\tlocation (str): The city and state, e.g. Madrid, Barcelona\n",
        "\tunit (str): The unit. It can take two values; \"celsius\", \"fahrenheit\"\n",
        "\t\"\"\"\n",
        "\tif location == \"Madrid\":\n",
        "\t\treturn json.dumps({\"temperature\": 25, \"unit\": unit})\n",
        "\n",
        "\telse:\n",
        "\t\treturn json.dumps({\"temperature\": 58, \"unit\": unit})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de31cb35-847f-458f-b7d7-603acf5a714a",
      "metadata": {
        "id": "de31cb35-847f-458f-b7d7-603acf5a714a"
      },
      "source": [
        "Very simple, right? You provide a `location` and a `unit` and it returns the temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3f52e61e-be31-4e6f-9f4f-eeb7082ad827",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3f52e61e-be31-4e6f-9f4f-eeb7082ad827",
        "outputId": "25210c69-8111-4c6a-c20c-185421cfa34b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"temperature\": 25, \"unit\": \"celsius\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "get_current_weather(location=\"Madrid\", unit=\"celsius\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9d63a34-8a93-4551-a34a-a0e85c95aa6a",
      "metadata": {
        "id": "e9d63a34-8a93-4551-a34a-a0e85c95aa6a"
      },
      "source": [
        "But the question is:\n",
        "\n",
        "**How can you make this function available to an LLM?**\n",
        "\n",
        "An LLM is a type of NLP system, so it expects text as input. But how can we transform this function into text?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56a4f2f8-9fc2-4e3d-87cd-bdfca15e5ddc",
      "metadata": {
        "id": "56a4f2f8-9fc2-4e3d-87cd-bdfca15e5ddc"
      },
      "source": [
        "## A System Prompt that works"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93bed242-75ca-4ab7-a159-114a9e1e7e67",
      "metadata": {
        "id": "93bed242-75ca-4ab7-a159-114a9e1e7e67"
      },
      "source": [
        "For the LLM to be aware of this function, we need to provide some relevant information about it in the context. **I'm referring to the function name, attributes, description, etc.** Take a look at the following System Prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad89df0-d233-41cc-b002-19963e7740a1",
      "metadata": {
        "id": "1ad89df0-d233-41cc-b002-19963e7740a1"
      },
      "source": [
        "```xml\n",
        "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.\n",
        "You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug\n",
        "into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n",
        "For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
        "\n",
        "<tool_call>\n",
        "{\"name\": <function-name>,\"arguments\": <args-dict>}\n",
        "</tool_call>\n",
        "\n",
        "Here are the available tools:\n",
        "\n",
        "<tools> {\n",
        "    \"name\": \"get_current_weather\",\n",
        "    \"description\": \"Get the current weather in a given location location (str): The city and state, e.g. Madrid, Barcelona unit (str): The unit. It can take two values; 'celsius', 'fahrenheit'\",\n",
        "    \"parameters\": {\n",
        "        \"properties\": {\n",
        "            \"location\": {\n",
        "                \"type\": \"string\"\n",
        "            },\n",
        "            \"unit\": {\n",
        "                \"type\": \"string\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "</tools>\n",
        "```\n",
        "\n",
        "\n",
        "As you can see, the LLM enforces the LLM to behave as a `function calling AI model` who, given a list of function signatures inside the <tools></tools> XML tags\n",
        "will select which one to use. When the model decides a function to use, it will return a json like the following, representing a function call:\n",
        "\n",
        "```xml\n",
        "<tool_call>\n",
        "{\"name\": <function-name>,\"arguments\": <args-dict>}\n",
        "</tool_call>\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d2d8322-afc0-4469-90aa-23019bc929e7",
      "metadata": {
        "id": "0d2d8322-afc0-4469-90aa-23019bc929e7"
      },
      "source": [
        "Let's see how it works in practise! üëá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "692b5c16-77f3-4de0-b2b5-16bfc5812b7b",
      "metadata": {
        "id": "692b5c16-77f3-4de0-b2b5-16bfc5812b7b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from groq import Groq\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "\n",
        "# Remember to load the environment variables. You should have the Groq API Key in there :)\n",
        "load_dotenv()\n",
        "\n",
        "MODEL = \"llama-3.3-70b-versatile\"\n",
        "GROQ_CLIENT = Groq(api_key='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')\n",
        "\n",
        "# Define the System Prompt as a constant\n",
        "TOOL_SYSTEM_PROMPT = \"\"\"\n",
        "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.\n",
        "You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug\n",
        "into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n",
        "For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
        "\n",
        "<tool_call>\n",
        "{\"name\": <function-name>,\"arguments\": <args-dict>}\n",
        "</tool_call>\n",
        "\n",
        "Here are the available tools:\n",
        "\n",
        "<tools> {\n",
        "    \"name\": \"get_current_weather\",\n",
        "    \"description\": \"Get the current weather in a given location location (str): The city and state, e.g. Madrid, Barcelona unit (str): The unit. It can take two values; 'celsius', 'fahrenheit'\",\n",
        "    \"parameters\": {\n",
        "        \"properties\": {\n",
        "            \"location\": {\n",
        "                \"type\": \"str\"\n",
        "            },\n",
        "            \"unit\": {\n",
        "                \"type\": \"str\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "</tools>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0da45c0-0b4b-4153-83c7-eed1c312dcec",
      "metadata": {
        "id": "e0da45c0-0b4b-4153-83c7-eed1c312dcec"
      },
      "source": [
        "Let's ask a very simple question: `\"What's the current temperature in Madrid, in Celsius?\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e00b09e8-55d3-4a59-a9cf-29329af78d9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e00b09e8-55d3-4a59-a9cf-29329af78d9a",
        "outputId": "b8c4bd5b-8e17-4223-ab7c-21321de86231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tool_call>\n",
            "{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}}\n",
            "</tool_call>\n"
          ]
        }
      ],
      "source": [
        "tool_chat_history = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": TOOL_SYSTEM_PROMPT\n",
        "    }\n",
        "]\n",
        "agent_chat_history = []\n",
        "\n",
        "user_msg = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"What's the current temperature in Madrid, in Celsius?\"\n",
        "}\n",
        "\n",
        "tool_chat_history.append(user_msg)\n",
        "agent_chat_history.append(user_msg)\n",
        "\n",
        "output = GROQ_CLIENT.chat.completions.create(\n",
        "    messages=tool_chat_history,\n",
        "    model=MODEL\n",
        ").choices[0].message.content\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c26cf72-0b60-464e-9f83-af371a93b3d5",
      "metadata": {
        "id": "3c26cf72-0b60-464e-9f83-af371a93b3d5"
      },
      "source": [
        "---\n",
        "\n",
        "**That's an improvement!** We may not have the *proper* answer but, with this information, we can obtain it! How? Well, we just need to:\n",
        "\n",
        "1. Parse the LLM output. By this I mean deleting the XML tags\n",
        "2. Load the output as a proper Python dict\n",
        "\n",
        "The function below does exactly this.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4366ae38-055a-45ec-937b-dfec7eaad00b",
      "metadata": {
        "id": "4366ae38-055a-45ec-937b-dfec7eaad00b"
      },
      "outputs": [],
      "source": [
        "def parse_tool_call_str(tool_call_str: str):\n",
        "    pattern = r'</?tool_call>'\n",
        "    clean_tags = re.sub(pattern, '', tool_call_str)\n",
        "\n",
        "    try:\n",
        "        tool_call_json = json.loads(clean_tags)\n",
        "        return tool_call_json\n",
        "    except json.JSONDecodeError:\n",
        "        return clean_tags\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "        return \"There was some error parsing the Tool's output\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c5890ba4-3f2f-4dc8-9a62-dff0079f07bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5890ba4-3f2f-4dc8-9a62-dff0079f07bb",
        "outputId": "32cd483d-56b6-410a-9925-0c896d8a3cef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'get_current_weather',\n",
              " 'arguments': {'location': 'Madrid', 'unit': 'celsius'}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "parsed_output = parse_tool_call_str(output)\n",
        "parsed_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "944b0373-f647-423a-bf00-914ffb03dcd7",
      "metadata": {
        "id": "944b0373-f647-423a-bf00-914ffb03dcd7"
      },
      "source": [
        "We can simply run the function now, by passing the arguments like this üëá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "169f06bb-836d-4270-bd66-abc2aadc0757",
      "metadata": {
        "id": "169f06bb-836d-4270-bd66-abc2aadc0757"
      },
      "outputs": [],
      "source": [
        "result = get_current_weather(**parsed_output[\"arguments\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ecdfbbc5-7cdf-4c21-8b75-055446658675",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ecdfbbc5-7cdf-4c21-8b75-055446658675",
        "outputId": "1eea20c6-7260-4872-a438-c7ad4a8a5908"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"temperature\": 25, \"unit\": \"celsius\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "272a337d-c193-4316-bed5-bc1ee4ccaae5",
      "metadata": {
        "id": "272a337d-c193-4316-bed5-bc1ee4ccaae5"
      },
      "source": [
        "**That's it!** A temperature of 25 degrees Celsius.\n",
        "\n",
        "As you can see, we're dealing with a string, so we can simply add the parsed_output to the `chat_history` so that the LLM knows the information it has to return to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3fb0fc08-dad9-42cd-a2a9-674b8191d06b",
      "metadata": {
        "id": "3fb0fc08-dad9-42cd-a2a9-674b8191d06b"
      },
      "outputs": [],
      "source": [
        "agent_chat_history.append({\n",
        "    \"role\": \"user\",\n",
        "    \"content\": f\"Observation: {result}\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b610fb1f-24af-4cc1-b485-fa0c5bfca846",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b610fb1f-24af-4cc1-b485-fa0c5bfca846",
        "outputId": "41175a5a-af87-4976-a6d7-ce20ffbe86fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature in Madrid is 25¬∞C.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "GROQ_CLIENT.chat.completions.create(\n",
        "    messages=agent_chat_history,\n",
        "    model=MODEL\n",
        ").choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72fa386e-edef-4e3f-903d-a2fc7008e5c3",
      "metadata": {
        "id": "72fa386e-edef-4e3f-903d-a2fc7008e5c3"
      },
      "source": [
        "## Implementing everything the good way"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4217eb34-efac-4a05-bb23-ae780126c0ad",
      "metadata": {
        "id": "4217eb34-efac-4a05-bb23-ae780126c0ad"
      },
      "source": [
        "To recap, we have a way for the LLM to generate `tool_calls` that we can use later to *properly* run the functions. But, as you may imagine, there are some pieces missing:\n",
        "\n",
        "1. We need to automatically transform any function into a description like we saw in the initial system prompt.\n",
        "2. We need a way to tell the agent that this function is a tool\n",
        "\n",
        "Let's do it!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df20db23-3c1a-4744-88b8-8d47d7875f18",
      "metadata": {
        "id": "df20db23-3c1a-4744-88b8-8d47d7875f18"
      },
      "source": [
        "### The `tool` decorator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c538804-a381-4552-94eb-c04720e897df",
      "metadata": {
        "id": "7c538804-a381-4552-94eb-c04720e897df"
      },
      "source": [
        "We are going to use the `tool` decorator to transform any Python function into a tool. You can see the implementation [here](https://github.com/neural-maze/agentic_patterns/blob/main/src/agentic_patterns/tool_pattern/tool.py). To test it out, let's make a more complex tool than before. For example, a tool that interacts with [Hacker News](https://news.ycombinator.com/), getting the current top stories.\n",
        "\n",
        "> Reminder: To automatically generate the function signature for the tool, we need a way to infer the arguments types. For this reason, we need to create the typing annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b9413902-e3ea-4c0a-bfd2-180d69ba5cd1",
      "metadata": {
        "id": "b9413902-e3ea-4c0a-bfd2-180d69ba5cd1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "from agentic_patterns.tool_pattern.tool import tool\n",
        "from agentic_patterns.tool_pattern.tool_agent import ToolAgent\n",
        "\n",
        "def fetch_top_hacker_news_stories(top_n: int):\n",
        "    \"\"\"\n",
        "    Fetch the top stories from Hacker News.\n",
        "\n",
        "    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API.\n",
        "    Each story contains the title, URL, score, author, and time of submission. The data is fetched\n",
        "    from the official Firebase Hacker News API, which returns story details in JSON format.\n",
        "\n",
        "    Args:\n",
        "        top_n (int): The number of top stories to retrieve.\n",
        "    \"\"\"\n",
        "    top_stories_url = 'https://hacker-news.firebaseio.com/v0/topstories.json'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(top_stories_url)\n",
        "        response.raise_for_status()  # Check for HTTP errors\n",
        "\n",
        "        # Get the top story IDs\n",
        "        top_story_ids = response.json()[:top_n]\n",
        "\n",
        "        top_stories = []\n",
        "\n",
        "        # For each story ID, fetch the story details\n",
        "        for story_id in top_story_ids:\n",
        "            story_url = f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json'\n",
        "            story_response = requests.get(story_url)\n",
        "            story_response.raise_for_status()  # Check for HTTP errors\n",
        "            story_data = story_response.json()\n",
        "\n",
        "            # Append the story title and URL (or other relevant info) to the list\n",
        "            top_stories.append({\n",
        "                'title': story_data.get('title', 'No title'),\n",
        "                'url': story_data.get('url', 'No URL available'),\n",
        "            })\n",
        "\n",
        "        return json.dumps(top_stories)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73f75359-1e8a-4317-92dd-40dd1cf36e97",
      "metadata": {
        "id": "73f75359-1e8a-4317-92dd-40dd1cf36e97"
      },
      "source": [
        "If we run this Python function, we'll obtain the top HN stories, as you can see below (the top 5 in this case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "aad2bbed-549e-4c0e-91fd-37b4694e0b50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aad2bbed-549e-4c0e-91fd-37b4694e0b50",
        "outputId": "cf3af709-c679-497b-a830-83981d212f18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'DeepSeek: X2 Speed for WASM with SIMD',\n",
              "  'url': 'https://simonwillison.net/2025/Jan/27/llamacpp-pr/'},\n",
              " {'title': 'DeepSeek improved the Transformer architecture',\n",
              "  'url': 'https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture'},\n",
              " {'title': 'New Speculative Attacks on Apple CPUs',\n",
              "  'url': 'https://predictors.fail/'},\n",
              " {'title': 'Using UV as Your Shebang Line',\n",
              "  'url': 'https://akrabat.com/using-uv-as-your-shebang-line/'},\n",
              " {'title': 'Maxima in the browser using Embedded Common Lisp on WASM',\n",
              "  'url': 'https://maxima-on-wasm.pages.dev/'}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "json.loads(fetch_top_hacker_news_stories(top_n=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb587d13-b312-45b5-af56-4f009c11eeda",
      "metadata": {
        "id": "fb587d13-b312-45b5-af56-4f009c11eeda"
      },
      "source": [
        "To transform the `fetch_top_hacker_news_stories` function into a Tool, we can use the `tool` decorator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4616e412-d4a8-4fe5-bcb1-dd00ce48640a",
      "metadata": {
        "id": "4616e412-d4a8-4fe5-bcb1-dd00ce48640a"
      },
      "outputs": [],
      "source": [
        "hn_tool = tool(fetch_top_hacker_news_stories)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f438638-a933-414f-9d00-53c37f041f16",
      "metadata": {
        "id": "3f438638-a933-414f-9d00-53c37f041f16"
      },
      "source": [
        "The Tool has the following parameters: a `name`, a `fn_signature` and the `fn` (this is the function we are going to call, this case `fetch_top_hacker_news_stories`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "df16bfa5-0ed4-46e1-b262-006f36fb8e78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "df16bfa5-0ed4-46e1-b262-006f36fb8e78",
        "outputId": "4e51051e-d201-4059-abab-b18912d2f626"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fetch_top_hacker_news_stories'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "hn_tool.name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3209e3e0-b59c-4b0e-b075-8fcbf9d21516",
      "metadata": {
        "id": "3209e3e0-b59c-4b0e-b075-8fcbf9d21516"
      },
      "source": [
        "By default, the tool gets its name from the function name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e0da95e0-10a8-4d17-aae7-ed3cc20abb03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0da95e0-10a8-4d17-aae7-ed3cc20abb03",
        "outputId": "e2eb6f1f-1adc-4c5e-a925-b6e07e6b030f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'fetch_top_hacker_news_stories',\n",
              " 'description': '\\n    Fetch the top stories from Hacker News.\\n\\n    This function retrieves the top `top_n` stories from Hacker News using the Hacker News API. \\n    Each story contains the title, URL, score, author, and time of submission. The data is fetched \\n    from the official Firebase Hacker News API, which returns story details in JSON format.\\n\\n    Args:\\n        top_n (int): The number of top stories to retrieve.\\n    ',\n",
              " 'parameters': {'properties': {'top_n': {'type': 'int'}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "json.loads(hn_tool.fn_signature)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5760bf7-7d9a-4c79-bc87-6469040250b6",
      "metadata": {
        "id": "d5760bf7-7d9a-4c79-bc87-6469040250b6"
      },
      "source": [
        "As you can see, the function signature has been automatically generated. It contains the `name`, a `description` (taken from the docstrings) and the `parameters`, whose types come from the tying annotations. Now that we have a tool, let's run the agent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "043ad8ba-7789-468a-aafd-60c10bd21135",
      "metadata": {
        "id": "043ad8ba-7789-468a-aafd-60c10bd21135"
      },
      "source": [
        "### The `ToolAgent`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065e04e5-50af-4452-9086-eae08a12e8cf",
      "metadata": {
        "id": "065e04e5-50af-4452-9086-eae08a12e8cf"
      },
      "source": [
        "To create the agent, we just need to pass a list of tools (in this case, just one)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "from colorama import Fore\n",
        "from dotenv import load_dotenv\n",
        "from groq import Groq\n",
        "\n",
        "from agentic_patterns.tool_pattern.tool import Tool\n",
        "from agentic_patterns.tool_pattern.tool import validate_arguments\n",
        "from agentic_patterns.utils.completions import build_prompt_structure\n",
        "from agentic_patterns.utils.completions import ChatHistory\n",
        "from agentic_patterns.utils.completions import completions_create\n",
        "from agentic_patterns.utils.completions import update_chat_history\n",
        "from agentic_patterns.utils.extraction import extract_tag_content\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "TOOL_SYSTEM_PROMPT = \"\"\"\n",
        "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.\n",
        "You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug\n",
        "into functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n",
        "For each function call return a json object with function name and arguments within <tool_call></tool_call>\n",
        "XML tags as follows:\n",
        "\n",
        "<tool_call>\n",
        "{\"name\": <function-name>,\"arguments\": <args-dict>,  \"id\": <monotonically-increasing-id>}\n",
        "</tool_call>\n",
        "\n",
        "Here are the available tools:\n",
        "\n",
        "<tools>\n",
        "%s\n",
        "</tools>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ToolAgent:\n",
        "    \"\"\"\n",
        "    The ToolAgent class represents an agent that can interact with a language model and use tools\n",
        "    to assist with user queries. It generates function calls based on user input, validates arguments,\n",
        "    and runs the respective tools.\n",
        "\n",
        "    Attributes:\n",
        "        tools (Tool | list[Tool]): A list of tools available to the agent.\n",
        "        model (str): The model to be used for generating tool calls and responses.\n",
        "        client (Groq): The Groq client used to interact with the language model.\n",
        "        tools_dict (dict): A dictionary mapping tool names to their corresponding Tool objects.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tools: Tool | list[Tool],\n",
        "        model: str = \"llama-3.3-70b-versatile\",\n",
        "    ) -> None:\n",
        "        self.client = Groq(api_key='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')\n",
        "        self.model = model\n",
        "        self.tools = tools if isinstance(tools, list) else [tools]\n",
        "        self.tools_dict = {tool.name: tool for tool in self.tools}\n",
        "\n",
        "    def add_tool_signatures(self) -> str:\n",
        "        \"\"\"\n",
        "        Collects the function signatures of all available tools.\n",
        "\n",
        "        Returns:\n",
        "            str: A concatenated string of all tool function signatures in JSON format.\n",
        "        \"\"\"\n",
        "        return \"\".join([tool.fn_signature for tool in self.tools])\n",
        "\n",
        "    def process_tool_calls(self, tool_calls_content: list) -> dict:\n",
        "        \"\"\"\n",
        "        Processes each tool call, validates arguments, executes the tools, and collects results.\n",
        "\n",
        "        Args:\n",
        "            tool_calls_content (list): List of strings, each representing a tool call in JSON format.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary where the keys are tool call IDs and values are the results from the tools.\n",
        "        \"\"\"\n",
        "        observations = {}\n",
        "        for tool_call_str in tool_calls_content:\n",
        "            tool_call = json.loads(tool_call_str)\n",
        "            tool_name = tool_call[\"name\"]\n",
        "            tool = self.tools_dict[tool_name]\n",
        "\n",
        "            print(Fore.GREEN + f\"\\nUsing Tool: {tool_name}\")\n",
        "\n",
        "            # Validate and execute the tool call\n",
        "            validated_tool_call = validate_arguments(\n",
        "                tool_call, json.loads(tool.fn_signature)\n",
        "            )\n",
        "            print(Fore.GREEN + f\"\\nTool call dict: \\n{validated_tool_call}\")\n",
        "\n",
        "            result = tool.run(**validated_tool_call[\"arguments\"])\n",
        "            print(Fore.GREEN + f\"\\nTool result: \\n{result}\")\n",
        "\n",
        "            # Store the result using the tool call ID\n",
        "            observations[validated_tool_call[\"id\"]] = result\n",
        "\n",
        "        return observations\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        user_msg: str,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Handles the full process of interacting with the language model and executing a tool based on user input.\n",
        "\n",
        "        Args:\n",
        "            user_msg (str): The user's message that prompts the tool agent to act.\n",
        "\n",
        "        Returns:\n",
        "            str: The final output after executing the tool and generating a response from the model.\n",
        "        \"\"\"\n",
        "        user_prompt = build_prompt_structure(prompt=user_msg, role=\"user\")\n",
        "\n",
        "        tool_chat_history = ChatHistory(\n",
        "            [\n",
        "                build_prompt_structure(\n",
        "                    prompt=TOOL_SYSTEM_PROMPT % self.add_tool_signatures(),\n",
        "                    role=\"system\",\n",
        "                ),\n",
        "                user_prompt,\n",
        "            ]\n",
        "        )\n",
        "        agent_chat_history = ChatHistory([user_prompt])\n",
        "\n",
        "        tool_call_response = completions_create(\n",
        "            self.client, messages=tool_chat_history, model=self.model\n",
        "        )\n",
        "        tool_calls = extract_tag_content(str(tool_call_response), \"tool_call\")\n",
        "\n",
        "        if tool_calls.found:\n",
        "            observations = self.process_tool_calls(tool_calls.content)\n",
        "            update_chat_history(\n",
        "                agent_chat_history, f'f\"Observation: {observations}\"', \"user\"\n",
        "            )\n",
        "\n",
        "        return completions_create(self.client, agent_chat_history, self.model)\n"
      ],
      "metadata": {
        "id": "g92XvOsOMtKG"
      },
      "id": "g92XvOsOMtKG",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4a303211-f2a6-43c0-85aa-081fb0be2bbe",
      "metadata": {
        "id": "4a303211-f2a6-43c0-85aa-081fb0be2bbe"
      },
      "outputs": [],
      "source": [
        "tool_agent = ToolAgent(tools=[hn_tool])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9eabae6-2b5c-407e-9e43-88e5e4844e9e",
      "metadata": {
        "id": "b9eabae6-2b5c-407e-9e43-88e5e4844e9e"
      },
      "source": [
        "A quick check to see that everything works fine. If we ask the agent something unrelated to Hacker News, it shouldn't use the tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "92c706fd-0a4b-46be-bbb3-c02618dbf677",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "92c706fd-0a4b-46be-bbb3-c02618dbf677",
        "outputId": "47f04ee8-ca98-44b4-e0ca-c36a79933d9a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'None'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_4352/402797296.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Tell me your name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_4352/3485188117.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, user_msg)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtool_calls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_tool_calls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_calls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             update_chat_history(\n\u001b[1;32m    136\u001b[0m                 \u001b[0magent_chat_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'f\"Observation: {observations}\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_4352/3485188117.py\u001b[0m in \u001b[0;36mprocess_tool_calls\u001b[0;34m(self, tool_calls_content)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mtool_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_call_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtool_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool_call\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mtool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtool_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"\\nUsing Tool: {tool_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'None'"
          ]
        }
      ],
      "source": [
        "output = tool_agent.run(user_msg=\"Tell me your name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be02a976-1e72-40ad-9ada-460148ca65d1",
      "metadata": {
        "id": "be02a976-1e72-40ad-9ada-460148ca65d1"
      },
      "outputs": [],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c862a34-3cc9-428b-a246-d98effc998a5",
      "metadata": {
        "id": "6c862a34-3cc9-428b-a246-d98effc998a5"
      },
      "source": [
        "Now, let's ask for specific information about Hacker News."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b74bbc64-8943-4ae3-9928-6230ead61e77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b74bbc64-8943-4ae3-9928-6230ead61e77",
        "outputId": "c3a539f5-16c9-459f-a364-f5477d933a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\n",
            "Using Tool: fetch_top_hacker_news_stories\n",
            "\u001b[32m\n",
            "Tool call dict: \n",
            "{'name': 'fetch_top_hacker_news_stories', 'arguments': {'top_n': 5}, 'id': 1}\n",
            "\u001b[32m\n",
            "Tool result: \n",
            "[{\"title\": \"DeepSeek: X2 Speed for WASM with SIMD\", \"url\": \"https://simonwillison.net/2025/Jan/27/llamacpp-pr/\"}, {\"title\": \"How has DeepSeek improved the Transformer architecture?\", \"url\": \"https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture\"}, {\"title\": \"New Speculative Attacks on Apple CPUs\", \"url\": \"https://predictors.fail/\"}, {\"title\": \"Using UV as Your Shebang Line\", \"url\": \"https://akrabat.com/using-uv-as-your-shebang-line/\"}, {\"title\": \"Maxima in the browser using Embedded Common Lisp on WASM\", \"url\": \"https://maxima-on-wasm.pages.dev/\"}]\n"
          ]
        }
      ],
      "source": [
        "output = tool_agent.run(user_msg=\"Tell me the top 5 Hacker News stories right now\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "53476bff-812d-4e56-afb9-de21474f6580",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53476bff-812d-4e56-afb9-de21474f6580",
        "outputId": "aebfb08c-bf49-4cd1-e06d-1f0b4990d04e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided data, here are the top 5 Hacker News stories right now:\n",
            "\n",
            "1. **DeepSeek: X2 Speed for WASM with SIMD** - https://simonwillison.net/2025/Jan/27/llamacpp-pr/\n",
            "2. **How has DeepSeek improved the Transformer architecture?** - https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture\n",
            "3. **New Speculative Attacks on Apple CPUs** - https://predictors.fail/\n",
            "4. **Using UV as Your Shebang Line** - https://akrabat.com/using-uv-as-your-shebang-line/\n",
            "5. **Maxima in the browser using Embedded Common Lisp on WASM** - https://maxima-on-wasm.pages.dev/\n",
            "\n",
            "Please note that the ranking is not explicitly provided in the data, so this is just an ordered list of the stories. If you need the actual ranking or scores, you would need to retrieve that information from the Hacker News API or website.\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f70a8059-9637-45b5-8050-ea7ba4995407",
      "metadata": {
        "id": "f70a8059-9637-45b5-8050-ea7ba4995407"
      },
      "source": [
        "---\n",
        "There you have it!! A fully functional Tool!! üõ†Ô∏è"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yzAYCAykN4e-"
      },
      "id": "yzAYCAykN4e-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}